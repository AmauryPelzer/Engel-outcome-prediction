{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def find_latest_t1w_scan(base_path):\n",
    "    participant_scans = {}\n",
    "    for item in os.listdir(base_path):\n",
    "        # Check if the folder name starts with 'sub-', indicating a participant folder\n",
    "        if item.startswith('sub-'):\n",
    "            participant_path = os.path.join(base_path, item)\n",
    "            # Strip 'sub-' prefix to use as the dictionary key\n",
    "            participant_id = item[4:]  # Removes the first four characters 'sub-'\n",
    "            latest_date = None\n",
    "            latest_file = None\n",
    "\n",
    "            # Check each session folder within the participant directory\n",
    "            for session_folder in os.listdir(participant_path):\n",
    "                if session_folder.startswith('ses-'):\n",
    "                    session_date = session_folder.split('-')[1]  # Extract the date from the session folder name\n",
    "                    session_path = os.path.join(participant_path, session_folder)\n",
    "                    anat_path = os.path.join(session_path, 'anat')  # 'anat' folder inside the session folder\n",
    "\n",
    "                    if os.path.exists(anat_path) and os.path.isdir(anat_path):\n",
    "                        # Iterate over all .nii files in the 'anat' directory\n",
    "                        for file in os.listdir(anat_path):\n",
    "                            if file.endswith('T1w.nii'):\n",
    "                                file_date = datetime.strptime(session_date, \"%Y%m%d\")\n",
    "                                # Update if this file's date is more recent\n",
    "                                if latest_date is None or file_date > latest_date:\n",
    "                                    latest_date = file_date\n",
    "                                    latest_file = os.path.join(anat_path, file)\n",
    "\n",
    "            if latest_file:\n",
    "                participant_scans[participant_id] = latest_file\n",
    "\n",
    "    return participant_scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../../data/raw/resectMap_nifti_only_20240430'\n",
    "latest_scans = find_latest_t1w_scan(base_path)\n",
    "#latest_scans.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prediction data\n",
    "prediction_data = pd.read_csv('../../data/processed/label_df.csv', index_col=0)\n",
    "\n",
    "# Convert latest_scans dictionary to DataFrame\n",
    "scans_df = pd.DataFrame(list(latest_scans.items()), columns=['ParticipantID', 'ScanPath'])\n",
    "\n",
    "# Merge the dataframes\n",
    "prediction_data = prediction_data.rename(columns={\"record_id\" : \"ParticipantID\"})\n",
    "final_data = prediction_data.merge(scans_df, on='ParticipantID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('../../data/processed/MRI_file_path.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "def load_mri(path):\n",
    "    mri = nib.load(path)\n",
    "    return mri.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_mri(data):\n",
    "    # Normalize the data to [0, 1]\n",
    "    data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = final_data.dropna(subset=['ScanPath']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant ID: RSCT000111, Shape: (192, 256, 256)\n",
      "Participant ID: RSCT000208, Shape: (192, 256, 256)\n",
      "Participant ID: RSCT000508, Shape: (240, 256, 256)\n",
      "Participant ID: RSCT000749, Shape: (256, 256, 256)\n",
      "Participant ID: RSCT001112, Shape: (192, 256, 256)\n",
      "Participant ID: RSCT001207, Shape: (192, 256, 256)\n",
      "Participant ID: RSCT001300, Shape: (192, 256, 256)\n",
      "Participant ID: RSCT001425, Shape: (256, 256, 256)\n",
      "Participant ID: RSCT001613, Shape: (192, 256, 256)\n",
      "Participant ID: RSCT001733, Shape: (160, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "for _, row in subset_df.iterrows():\n",
    "    mri_data = load_mri(row['ScanPath'])\n",
    "    print(f\"Participant ID: {row['ParticipantID']}, Shape: {mri_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem : MRIs with different shapes -> need resize\n",
    "if False:\n",
    "    X = []  # Image data\n",
    "    y = []  # Labels\n",
    "\n",
    "    # Remove participants without any T1w MRI scans\n",
    "    final_data = final_data.dropna(subset=['ScanPath'])\n",
    "\n",
    "    for _, row in subset_df.iterrows():\n",
    "        mri_data = load_mri(row['ScanPath'])\n",
    "        mri_data = preprocess_mri(mri_data)\n",
    "        X.append(mri_data)\n",
    "        y.append(row['surg_engel'])\n",
    "\n",
    "    X = np.array(X)  # Convert list to array for training\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def resize_mri(data, new_shape=(64, 64, 64)):\n",
    "    \"\"\" Resize the MRI to new_shape \"\"\"\n",
    "    # Calculate the zoom factors\n",
    "    zoom_factors = np.array(new_shape) / np.array(data.shape)\n",
    "    # Apply the zoom operation with bilinear interpolation\n",
    "    return zoom(data, zoom_factors, order=1)  # order=1 (bilinear) is often a good trade-off\n",
    "\n",
    "\n",
    "def preprocess_and_load_mris(df):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        mri_data = load_mri(row['ScanPath'])\n",
    "        mri_data = preprocess_mri(mri_data)\n",
    "        mri_data_resized = resize_mri(mri_data)\n",
    "        X.append(mri_data_resized)\n",
    "        y.append(row['surg_engel'])\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assume df is already defined and loaded with paths and outcomes\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_and_load_mris\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 18\u001b[0m, in \u001b[0;36mpreprocess_and_load_mris\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     15\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 18\u001b[0m     mri_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_mri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mScanPath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     mri_data \u001b[38;5;241m=\u001b[39m preprocess_mri(mri_data)\n\u001b[0;32m     20\u001b[0m     mri_data_resized \u001b[38;5;241m=\u001b[39m resize_mri(mri_data)\n",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m, in \u001b[0;36mload_mri\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_mri\u001b[39m(path):\n\u001b[1;32m----> 4\u001b[0m     mri \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mri\u001b[38;5;241m.\u001b[39mget_fdata()\n",
      "File \u001b[1;32mc:\\Users\\amaur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\loadsave.py:96\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(filename: FileSpec, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FileBasedImage:\n\u001b[0;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Load file given filename, guessing at file type\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m       Image of guessed type\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43m_stringify_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# Check file exists and is not empty\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\amaur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\filename_parser.py:41\u001b[0m, in \u001b[0;36m_stringify_path\u001b[1;34m(filepath_or_buffer)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stringify_path\u001b[39m(filepath_or_buffer: FileSpec) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Attempt to convert a path-like object to a string.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    https://github.com/pandas-dev/pandas/blob/325dd68/pandas/io/common.py#L131-L160\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mas_posix()\n",
      "File \u001b[1;32mc:\\Users\\amaur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pathlib.py:871\u001b[0m, in \u001b[0;36mPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Path:\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[1;32m--> 871\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n",
      "File \u001b[1;32mc:\\Users\\amaur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pathlib.py:509\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[1;34m(cls, args)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_parts\u001b[39m(\u001b[38;5;28mcls\u001b[39m, args):\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# We need to call _parse_args on the instance, so as to get the\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;66;03m# right flavour.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m--> 509\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m root\n",
      "File \u001b[1;32mc:\\Users\\amaur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pathlib.py:493\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[1;34m(cls, args)\u001b[0m\n\u001b[0;32m    491\u001b[0m     parts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39m_parts\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 493\u001b[0m     a \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(a)\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    495\u001b[0m         \u001b[38;5;66;03m# Force-cast str subclasses to str (issue #21127)\u001b[39;00m\n\u001b[0;32m    496\u001b[0m         parts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(a))\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not float"
     ]
    }
   ],
   "source": [
    "X, y = preprocess_and_load_mris(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
    "\n",
    "def create_model(input_shape):\n",
    "    \"\"\" Create a 3D CNN model. \"\"\"\n",
    "    model = Sequential([\n",
    "        Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[..., np.newaxis]  # Add a channel dimension, assuming X doesn't already have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X.shape[1:]\n",
    "model = create_model(input_shape)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss}, Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(X.shape[1], X.shape[2], X.shape[3], 1)),\n",
    "    MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=10, batch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "class MRISequence(Sequence):\n",
    "    def __init__(self, df, batch_size):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.df.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X = []\n",
    "        y = []\n",
    "        for _, row in batch_x.iterrows():\n",
    "            mri_data = load_mri(row['ScanPath'])\n",
    "            mri_data = preprocess_mri(mri_data)\n",
    "            mri_data_resized = resize_mri(mri_data)\n",
    "            X.append(mri_data_resized)\n",
    "            y.append(row['Outcome'])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "# Usage\n",
    "batch_size = 2  # You can adjust the batch size\n",
    "train_gen = MRISequence(df=subset_df, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
    "\n",
    "def create_model(input_shape):\n",
    "    # Create a Sequential model\n",
    "    model = Sequential([\n",
    "        Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Assuming your input shape from the preprocessed MRI data is known, e.g., (64, 64, 64, 1)\n",
    "input_shape = (64, 64, 64, 1)\n",
    "model = create_model(input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_gen, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print system information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "\n",
    "session_info.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
