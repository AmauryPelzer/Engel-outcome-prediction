{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of clinical data\n",
    "\n",
    "Preprocessing of the clinical data from the EHR (electronic health record).\n",
    "\n",
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amaury\\AppData\\Local\\Temp\\ipykernel_28428\\79705969.py:3: DtypeWarning: Columns (8,20,31,39,65,79,83,85,127,130,132,150,153,161,308,313,317,321,325,329,330,334,377,385,403,409,439,440,441,442,443,444,445,453,463,475,512,523,524,525,526,527,528,529,537,566,572,574,588,589,590,591,592,601,602,604,605,606,608,620,621,622,627,633,636,637,639,646,647,658,676,677,682,695,698,712,714,718,720,721,723,725,726,728,732,733,735,748,750,754,757,759,761,762,764,768,770,772,774,776,780,782,786,787,791,796,803,810,812,813,814,815,817,818,880,887,888,896,904,905,906,968,975,976,984,992,993,994,1056,1063,1064,1072,1080,1081,1082,1144,1151,1152,1169,1170,1232,1239,1240,1257,1258,1320,1327,1328,1345,1408,1415,1416,1432,1433,1496,1503,1521,1523,1540,1541,1558,1559,1576,1577,1595,1613,1631,1649,1667,1669,1670,1671,1678,1730,1738,1790,1801,1802,1811,1819,1820,1821,1822,1823,1830,1882,1890,1942,1953,1954,1963,1971,1972,1973,1974,1975,1982,2034,2042,2094,2105,2106,2115,2124,2125,2126,2127,2134,2186,2194,2246,2257,2258,2267,2276,2277,2278,2279,2286,2338,2346,2398,2409,2410,2419,2427,2428,2429,2430,2431,2438,2490,2498,2550,2561,2562,2580,2581,2582,2583,2590,2642,2650,2702,2713,2714,2731,2732,2733,2734,2735,2742,2794,2802,2854,2865,2866,2883,2884,2885,2886,2887,2888,2889,2899,2903,2912,2917,2929,2951,2977,2978,3116,3128,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3150,3156,3159,3162,3165,3167,3173,3176,3183,3244,3300,3305,3307,3308,3311,3314,3316,3325,3332,3393,3449,3478,3479,3483,3488,3492,3495,3497,3498,3499,3500,3501,3502,3504,3505,3506,3507,3508,3509,3511,3512,3513,3514,3515,3518,3519,3520,3521,3522,3543,3550,3551,3564,3566,3588,3589,3592,3593,3594,3595,3596,3597,3598,3599,3600,3601,3602,3603,3605,3606,3607,3608,3609,3612,3613,3614,3615,3616,3617,3618,3619,3620,3621,3622,3623,3624,3625,3626,3627,3628,3629,3630,3631,3632,3633,3634,3635,3636,3637,3638,3639,3640,3641,3642,3643,3644,3645,3646,3647,3650,3651,3652,3653,3654,3655,3656,3657,3660,3661,3662,3663,3672,3673,3679,3680,3681,3684,3685,3688,3690,3695,3696,3698,3712,3718,3719,3726,3727,3728,3733,3734,3737,3738,3741,3744,3745,3748,3763,3779,3790,3796,3797,3798,3799,3800,3801,3802,3803,3804,3805,3806,3807,3808,3809,3810,3811,3812,3813,3814,3815,3816,3817,3818,3819,3820,3821,3822,3823,3824,3825,3826,3827,3828,3829,3830,3831,3832,3833,3834,3835,3836,3837,3841,3844,3847,3848,3852,3853,3854,3855,3856,3857,3858,3859,3860,3861,3862,3863,3864,3867,3871,3872,3874,3876,3877,3883,3885,3886,3887,3888,3889,3890,3893,3894,3907,3917,3925,3929,3937,3939,3945,3946,3949,3950,3951,3952,3953,3954,3955,3956,3957,3958,3959,3960,3961,3963,3965,3970,3971,3972,3973,3976,3977,3978,3979,3981,3982,3983,3986,3995,3996,3997,3998,3999,4000,4001,4002,4003,4004,4005,4006,4007,4008,4009,4010,4011,4012,4013,4014,4015,4016,4017,4018,4019,4020,4021,4022,4023,4024,4025,4026,4027,4028,4029,4030,4031,4032,4033,4034,4035,4036,4038,4042,4043,4044,4045,4046,4047,4048,4049,4050,4051,4052,4053,4054,4055,4056,4057,4058,4059,4060,4061,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4091,4093,4095,4104,4105,4106,4107,4108,4109,4110,4111,4112,4113,4114,4115,4116,4117,4119,4120,4121,4122,4123,4124,4125,4126,4127,4128,4129,4130,4131,4132,4133,4134,4135,4136,4137,4138,4139,4140,4141,4142,4143,4144,4145,4146,4147,4148,4149,4150,4151,4152,4153,4154,4155,4157,4158,4159,4160,4161,4162,4163,4164,4165,4166,4167,4168,4169,4170,4172,4173,4174,4175,4176,4178,4179,4180,4181,4182,4183,4184,4185,4186,4187,4189,4190,4191,4192,4193,4194,4195,4196,4197,4198,4199,4200,4201,4202,4203,4204,4205,4206,4207,4208,4209,4210,4211,4212,4213,4214,4215,4216,4217,4218,4219,4220,4221,4222,4223,4224,4226,4227,4228,4229,4230,4231,4232,4233,4234,4235,4236,4237,4238,4239,4240,4241,4242,4243,4244,4245,4246,4247,4248,4249,4250,4251,4255,4256,4257,4258,4259,4260,4261,4262,4263,4264,4265,4266,4267,4268,4269,4270,4271,4272,4273,4274,4275,4276,4277,4278,4279,4280,4281,4282,4283,4284,4285,4286,4287,4288,4289,4290,4291,4292,4293,4294,4295,4296,4297,4298,4303,4304,4305,4306,4307,4308,4309,4310,4311,4312,4313,4314,4315,4316,4317,4318,4319,4320,4321,4322,4323,4324,4325,4326,4327,4328,4329,4330,4331,4332,4333,4334,4335,4336,4337,4338,4339,4340,4341,4342,4343,4344,4345,4346,4347,4348,4349,4350,4351,4352,4353,4354,4355,4356,4357,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4376,4377,4378,4379,4380,4381,4382,4383,4384,4385,4386,4387,4388,4389,4390,4391,4392,4393,4394,4395,4396,4397,4398,4399,4400,4401,4402,4403,4404,4405,4406,4407,4408,4409,4410,4411,4412,4413,4414,4415,4416,4417,4418,4419,4420,4421,4422,4460,4461,4462,4463,4464,4465,4467,4468,4470,4471,4472,4473,4474,4475,4476,4477,4478,4479,4480,4481,4482,4483,4484,4485,4486,4487,4488,4489,4490,4491,4492,4493,4494,4495,4496,4497,4498,4499,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4527,4528,4530,4531,4532,4533,4534,4535,4536,4537,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4561,4562,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4600,4602,4603,4604,4605,4606,4607,4608,4609,4610,4611,4612,4613,4614,4615,4616,4617,4618,4619,4620,4621,4622,4623,4625,4626,4627,4628,4632,4633,4634,4635,4636,4637,4638,4639,4640,4641,4642,4643,4644,4658,4659,4660,4661,4662,4663,4664,4665,4666,4667,4668,4669,4670,4671,4672,4673,4674,4675) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../../data/raw/ResectMap_DATA_2023-11-13_2306.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data/raw/ResectMap_DATA_2023-11-13_2306.csv\")\n",
    "\n",
    "# Cell to set the flag variable\n",
    "execute_special_cell = False  # Set this to True to run the special cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Most Recent\" implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date of EO assessment as datetime format\n",
    "df['eo_date_clin'] = pd.to_datetime(df['eo_date_clin'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Only keep the most recent engel outcome assessment per 'record_id'\n",
    "engel_outcomes = df[df['redcap_repeat_instrument'] == 'engel_outcomes'][['record_id', 'eo_date_clin']]\n",
    "engel_outcomes = engel_outcomes.sort_values(by='eo_date_clin', ascending=False).drop_duplicates(subset='record_id', keep='first')\n",
    "\n",
    "# Merge this subset with the original dataframe on 'record_id' \n",
    "df = df.merge(engel_outcomes, on='record_id', suffixes=('', '_engel'), how='left')\n",
    "\n",
    "# Use transform to broadcast 'eo_date_surg' values to all rows per 'record_id'\n",
    "#df['eo_date_clin'] = df.groupby('record_id')['eo_date_clin'].transform('first')\n",
    "\n",
    "# Define the columns for each 'redcap_repeat_instrument' type\n",
    "date_columns = {\n",
    "    'surgical_information': 'resect_date',\n",
    "    'eeg': 'eeg_date',\n",
    "    'neuropsychological_testing': 'np_eval_date',\n",
    "    'mri': ['doi', 'doi2']  # Include both 'doi' and 'doi2' for 'mri'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date of EO assessment as datetime format\n",
    "#df['eo_date_surg_engel'] = pd.to_datetime(df['eo_date_surg_engel'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "for instrument, date_columns in date_columns.items():\n",
    "    # Create a subset of 'df' for the specific 'redcap_repeat_instrument' type\n",
    "    subset_info = df[df['redcap_repeat_instrument'] == instrument].copy()\n",
    "\n",
    "    # Convert the date column(s) from string format to datetime objects\n",
    "    if isinstance(date_columns, list):  # Check if there are multiple date columns (e.g., for 'mri')\n",
    "        for col in date_columns:\n",
    "            subset_info[col] = pd.to_datetime(subset_info[col], format='%m/%d/%Y', errors='coerce')\n",
    "        # If 'doi' is NaT (not a time), use 'doi2'\n",
    "        subset_info['final_date'] = subset_info[date_columns[0]].fillna(subset_info[date_columns[1]])\n",
    "    else:\n",
    "        subset_info[date_columns] = pd.to_datetime(subset_info[date_columns], format='%m/%d/%Y', errors='coerce')\n",
    "        subset_info['final_date'] = subset_info[date_columns]\n",
    "\n",
    "    # Calculate the absolute difference in days between the type-specific date and the date of EO assessment\n",
    "    subset_info['date_diff_abs'] = (subset_info['final_date'] - subset_info['eo_date_clin_engel']).dt.days.abs()\n",
    "    subset_info['date_diff'] = (subset_info['final_date'] - subset_info['eo_date_clin_engel']).dt.days\n",
    "\n",
    "    # Check if there are multiple rows for each 'record_id'\n",
    "    group_counts = subset_info.groupby('record_id')['date_diff'].transform('size')\n",
    "    multiple_rows_mask = group_counts > 1\n",
    "\n",
    "    # Rank the 'date_diff' within each 'record_id' group and assign the rank as 'new_instance' for groups with more than one row\n",
    "    #subset_info.loc[multiple_rows_mask, 'new_instance'] = subset_info[multiple_rows_mask].groupby('record_id')['date_diff'].rank(method='first')\n",
    "    subset_info.loc[multiple_rows_mask, 'new_instance'] = subset_info[multiple_rows_mask].groupby('record_id')['final_date'].rank(ascending=False)\n",
    "    \n",
    "    # For groups with only one row, set 'new_instance' to 1\n",
    "    subset_info.loc[~multiple_rows_mask, 'new_instance'] = 1\n",
    "\n",
    "    # Update 'redcap_repeat_instance' in the main dataframe\n",
    "    df.loc[subset_info.index, 'redcap_repeat_instance'] = subset_info['new_instance']\n",
    "    df.loc[subset_info.index, 'date_diff'] = subset_info['date_diff']\n",
    "    \n",
    "# Cleaning up\n",
    "#df.drop(columns=['eo_date_clin_engel', 'date_diff'], inplace=True, errors='ignore')\n",
    "\n",
    "df.to_csv(\"../../data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select only the most relevant information per patient\n",
    "\n",
    "To do so, we select the last surgical that is neither a shunt or implementing a stimulation device and use this date as reference.\n",
    "\n",
    "Afterwards, we select the eeg, neuropsych and mri examinations that happened closest to this last date of surgery reference.\n",
    "\n",
    "### Find date of last surgery for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ID and resect date\n",
    "surgery_dates = df.loc[df[\"redcap_repeat_instrument\"]==\"surgical_information\"]#[[\"record_id\", \"resect_date\"]]\n",
    "\n",
    "# Transform date column into datetime type/format\n",
    "surgery_dates[\"resect_date\"] = pd.to_datetime(surgery_dates[\"resect_date\"], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Specify columns to check for the condition (no shunt or stimulation)\n",
    "columns_to_check = [\n",
    "    \"resect_procedure___2\", \"resect_procedure___3\", \"resect_procedure___4\", \n",
    "    \"resect_procedure___5\", \"resect_procedure___6\", \"resect_procedure___7\", \n",
    "    \"resect_procedure___8\", \"resect_procedure___9\", \"resect_procedure___11\", \n",
    "    \"resect_procedure___12\", \"resect_procedure___13\"\n",
    "]\n",
    "\n",
    "# Remove shunt from being considered\n",
    "surgery_dates_no_shunt = surgery_dates.loc[df[columns_to_check].any(axis=1)]\n",
    "\n",
    "# Get most recent date within subset\n",
    "date_of_last_surgery = surgery_dates_no_shunt.groupby(\"record_id\")[\"resect_date\"].max().reset_index()\n",
    "date_of_last_surgery = date_of_last_surgery.rename(columns={\"resect_date\" : \"last_surg_date\"})\n",
    "\n",
    "# Check which participants don't have a resect date or only have shunts/stimulation\n",
    "missing_dates = set(df['record_id']) - set(date_of_last_surgery['record_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find eeg, neuropsych and mri closest to date of last surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the rows with 'engel_outcomes', sort by 'eo_date_surg' descending, and keep only the most recent occurrence per 'record_id'\n",
    "engel_outcomes = df[df['redcap_repeat_instrument'] == 'engel_outcomes'][['record_id', 'eo_date_surg']]\n",
    "engel_outcomes = engel_outcomes.sort_values(by='eo_date_surg', ascending=False).drop_duplicates(subset='record_id', keep='first')\n",
    "\n",
    "# Merge this subset with the original dataframe on 'record_id' \n",
    "df = df.merge(engel_outcomes, on='record_id', suffixes=('', '_engel'), how='left')\n",
    "\n",
    "# Use transform to broadcast 'eo_date_surg' values to all rows per 'record_id'\n",
    "df['eo_date_surg'] = df.groupby('record_id')['eo_date_surg'].transform('first')\n",
    "\n",
    "# Define the columns for each 'redcap_repeat_instrument' type\n",
    "date_columns = {\n",
    "    'surgical_information': 'resect_date',\n",
    "    'eeg': 'eeg_date',\n",
    "    'neuropsychological_testing': 'np_eval_date',\n",
    "    'mri': ['doi', 'doi2']  # Include both 'doi' and 'doi2' for 'mri'\n",
    "}\n",
    "\n",
    "for instrument, date_columns in date_columns.items():\n",
    "    # Create a subset of 'df' for the specific 'redcap_repeat_instrument' type\n",
    "    subset_info = df[df['redcap_repeat_instrument'] == instrument].copy()\n",
    "\n",
    "    # Convert the date column(s) from string format to datetime objects\n",
    "    if isinstance(date_columns, list):  # Check if there are multiple date columns (e.g., for 'mri')\n",
    "        for col in date_columns:\n",
    "            subset_info[col] = pd.to_datetime(subset_info[col], format='%m/%d/%Y', errors='coerce')\n",
    "        # If 'doi' is NaT (not a time), use 'doi2'\n",
    "        subset_info['final_date'] = subset_info[date_columns[0]].fillna(subset_info[date_columns[1]])\n",
    "    else:\n",
    "        subset_info[date_columns] = pd.to_datetime(subset_info[date_columns], format='%m/%d/%Y', errors='coerce')\n",
    "        subset_info['final_date'] = subset_info[date_columns]\n",
    "\n",
    "    subset_info['eo_date_surg'] = pd.to_datetime(subset_info['eo_date_surg'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "    # Calculate the absolute difference in days between the type-specific date and 'eo_date_surg'\n",
    "    subset_info['date_diff'] = (subset_info['final_date'] - subset_info['eo_date_surg']).dt.days.abs()\n",
    "\n",
    "    # Check if there are multiple rows for each 'record_id'\n",
    "    group_counts = subset_info.groupby('record_id')['date_diff'].transform('size')\n",
    "    multiple_rows_mask = group_counts > 1\n",
    "\n",
    "    # Rank the 'date_diff' within each 'record_id' group and assign the rank as 'new_instance' for groups with more than one row\n",
    "    subset_info.loc[multiple_rows_mask, 'new_instance'] = subset_info[multiple_rows_mask].groupby('record_id')['date_diff'].rank(method='first')\n",
    "\n",
    "    # For groups with only one row, set 'new_instance' to 1\n",
    "    subset_info.loc[~multiple_rows_mask, 'new_instance'] = 1\n",
    "\n",
    "    # Update 'redcap_repeat_instance' in the main dataframe\n",
    "    df.loc[subset_info.index, 'redcap_repeat_instance'] = subset_info['new_instance']\n",
    "    \n",
    "# Cleaning up\n",
    "df.drop(columns=['eo_date_surg_engel', 'date_diff'], inplace=True, errors='ignore')\n",
    "\n",
    "df.to_csv(\"../../data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the data\n",
    "\n",
    "1. Filter the dataframe for only the most recent information (redcap_repeat_instance == 1)\n",
    "2. Check that there is no overlapping information per participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill 'redcap_repeat_instance' with 1 only for rows where 'mrn' has a value\n",
    "df.loc[df['mrn'].notna(), 'redcap_repeat_instance'] = df.loc[df['mrn'].notna(), 'redcap_repeat_instance'].fillna(1)\n",
    "\n",
    "# Only keep rows where with only 1 measurement per instrument\n",
    "df_subset = df[df[\"redcap_repeat_instance\"]==1]\n",
    "\n",
    "columns_to_exclude = ['record_id', 'redcap_repeat_instrument', 'redcap_repeat_instance', \"eo_date_surg\"]\n",
    "\n",
    "# Get the list of columns to check by excluding the columns_to_exclude\n",
    "columns_to_check = [col for col in df_subset.columns if col not in columns_to_exclude]\n",
    "\n",
    "# Group by 'record_id' and check which columns have more than 1 row of information for each participant\n",
    "check_result = df_subset.groupby('record_id')[columns_to_check].apply(lambda x: x.columns[x.notnull().sum() > 1].tolist())\n",
    "\n",
    "# Print the result for each participant\n",
    "for record_id, problematic_columns in check_result.items():\n",
    "    if problematic_columns:\n",
    "        print(f\"Participant {record_id} has more than 1 row of information in columns: {', '.join(problematic_columns)}\")\n",
    "\n",
    "df_subset.to_csv(\"../../data/processed/test2.csv\")\n",
    "del columns_to_exclude, columns_to_check, check_result, record_id, problematic_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten the dataframe\n",
    "\n",
    "Flatten the dataframe from a long dataframe to a wide dataframe with only 1 particpant per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conditional cell execution\n",
    "if execute_special_cell==False:\n",
    "    \n",
    "    # Group by 'record_id' and aggregate non-null values for each column\n",
    "    wide_df = df_subset.groupby('record_id').agg(lambda x: x.dropna().iloc[0] if not x.dropna().empty else None)\n",
    "\n",
    "    # Reset the index to get a clean DataFrame\n",
    "    wide_df = wide_df.reset_index()\n",
    "\n",
    "    # Now, consolidated_df contains one row per patient with non-null information\n",
    "    print(wide_df)\n",
    "\n",
    "    # Save dataframe for further use\n",
    "    wide_df.to_csv(\"../../data/processed/wide_df.csv\")\n",
    "\n",
    "else:\n",
    "    # Read dataframe from the pickle format\n",
    "    wide_df = pd.read_csv(\"../../data/processed/wide_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for wrong datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = [\"redcap_repeat_instrument\", \"redcap_repeat_instance\", \"mrn\", \"study_site_other\", \"ny_num_yn\", \"ny_num\"]\n",
    "\n",
    "###### IN PROGRESS ######\n",
    "\n",
    "wide_df = wide_df.drop(columns_to_exclude, axis=1)\n",
    "\n",
    "del columns_to_exclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold (e.g., 0.5 for 50%)\n",
    "threshold = 0.9\n",
    "\n",
    "# Calculate the NaN percentage for each column\n",
    "nan_percentages = wide_df.isna().mean()\n",
    "\n",
    "# Count the columns where the NaN percentage exceeds the threshold\n",
    "columns_above_threshold = nan_percentages[nan_percentages > threshold].index.tolist()\n",
    "\n",
    "# Get the count of columns above the threshold\n",
    "count_above_threshold = len(columns_above_threshold)\n",
    "\n",
    "print(f\"Number of columns with NaN percentage above {threshold * 100}%: {count_above_threshold}\")\n",
    "\n",
    "del threshold, nan_percentages, count_above_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete columns with NA % above threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wide_df = wide_df.drop(columns_above_threshold, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new features\n",
    "\n",
    "### Number of measurements\n",
    "\n",
    "Because the data is filtered for only \"redcap_repeat_instance\" == 1, we lose the information on other timepoints.\n",
    "In order to capture that information, new variables/columns were created. These columns sum the number of measurement of each type per patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the dataframe\n",
    "pivoted_df = df.pivot_table(index='record_id', columns='redcap_repeat_instrument', values='redcap_repeat_instance', aggfunc='count')\n",
    "\n",
    "# Fill NaN values with 0\n",
    "pivoted_df.fillna(0, inplace=True)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(pivoted_df.head())\n",
    "\n",
    "# Merge the dataframes based on the 'record_id' column\n",
    "wide_df = pd.merge(wide_df, pivoted_df, on='record_id', how='left')\n",
    "\n",
    "del pivoted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age of seizure onset\n",
    "\n",
    "We extract the age of seizure onset from a text column (\"seizure_class_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_age_of_onset(text):\n",
    "    age_pattern = re.compile(r'''\n",
    "        (?:age\\s*of\\s*seizure\\s*onset|age\\s*at\\s*seizure\\s*onset|age\\s*of\\s*onset|\n",
    "         seizure\\s*onset\\s*age|seizure\\s*age\\s*of\\s*onset|seizure\\s*onset|\n",
    "         seizures*\\s*began\\s*(?:at|at\\s*the\\s*age\\s*of)*|(?:1st|first)\\s*seizure|\n",
    "         onset\\s*at\\s*age|age\\s*(?=:)|seizure\\s*onset\\s*:\\s*age)\\s*:*\\s*~*\\s*\n",
    "        (([0-9,.]+(?:\\s*(?:-|to|or)\\s*[0-9,.]+)?\\s*(?:y/o|years?|months?|days?|weeks?))|\n",
    "         in-utero|birth|(\\d+\\s*\\+\\s*\\d+\\s*months?)|\\d+|(?:\\d+\\s*(?:years?|yrs)\\s*\\d+\\s*months)|(?<=\\bage\\s)\\d+)  # age with optional units or range\n",
    "    ''', re.IGNORECASE | re.VERBOSE)\n",
    "\n",
    "    match = re.search(age_pattern, text)\n",
    "    if match:\n",
    "        age_with_units = match.group(1)\n",
    "        return convert_units_to_years(age_with_units)\n",
    "\n",
    "    # Handle specific phrases\n",
    "    if 'first seizure at birth' in text.lower() or 'seizures began at birth' in text.lower():\n",
    "        return 0\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def convert_units_to_years(age_with_units):\n",
    "    # Handle special cases for birth and in-utero\n",
    "    if 'birth' in age_with_units.lower():\n",
    "        return 0\n",
    "    if 'in-utero' in age_with_units.lower():\n",
    "        return 0\n",
    "\n",
    "    # Handle combined years and months (e.g., \"43yrs 11months\")\n",
    "    combined_pattern = re.match(r'(\\d+)\\s*(?:years?|yrs)\\s*(\\d+)\\s*months?', age_with_units, re.IGNORECASE)\n",
    "    if combined_pattern:\n",
    "        years, months = map(int, combined_pattern.groups())\n",
    "        return years + months / 12\n",
    "\n",
    "    # Handle range of ages\n",
    "    if '-' in age_with_units or 'to' in age_with_units or 'or' in age_with_units:\n",
    "        numbers = [float(n) for n in re.findall(r'\\d+(?:\\.\\d+)?', age_with_units)]\n",
    "        if numbers:\n",
    "            average_age = sum(numbers) / len(numbers)\n",
    "            return average_age / 12 if 'month' in age_with_units.lower() else average_age\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Regular age extraction\n",
    "    match = re.match(r'(\\d+(?:[.,]\\d+)?)\\s*(y/o|years?|months?|days?|weeks?|birth?|in-utero?)?', age_with_units, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        value, unit = match.groups()\n",
    "        if value:\n",
    "            value = float(value)\n",
    "            if unit:\n",
    "                if 'month' in unit.lower():\n",
    "                    return value / 12\n",
    "                elif 'week' in unit.lower():\n",
    "                    return value / 52.1775\n",
    "                elif 'day' in unit.lower():\n",
    "                    return value / 365.25\n",
    "                else:  # Assume it's years if no unit is specified\n",
    "                    return value\n",
    "            else:  # No unit specified, assume years\n",
    "                return value\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'doctor_notes' column and create a new 'age_of_onset' column\n",
    "wide_df['age_of_onset'] = wide_df['seizure_class_notes'].apply(lambda x: extract_age_of_onset(str(x)) if pd.notna(x) else None)\n",
    "\n",
    "result = wide_df[[\"age_of_onset\", \"seizure_class_notes\"]]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how many \"age at seizure onset\" the RE was able to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wide_df) - wide_df['age_of_onset'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how many \"age at seizure onset\" after manually checking the dataseet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional cell execution\n",
    "if execute_special_cell:\n",
    "\n",
    "    age_of_onset = wide_df[[\"record_id\", \"age_of_onset\", \"seizure_class_notes\"]]\n",
    "    age_of_onset = age_of_onset.rename(columns={\"age_of_onset\": \"age_of_onset_hw\"})\n",
    "    age_of_onset.to_csv(\"../../data/processed/age_of_onset_hw.csv\")\n",
    "    \n",
    "else:\n",
    "    # Read the hand-written DataFrame\n",
    "    age_of_onset_df = pd.read_csv(\"../../data/processed/age_of_onset_hw_done.csv\")\n",
    "\n",
    "    # Merge the hand-written data with wide_df\n",
    "    wide_df = pd.merge(wide_df, age_of_onset_df[['record_id', 'age_of_onset_hw']], on='record_id', how='left')\n",
    "\n",
    "    print(len(wide_df) - wide_df['age_of_onset_hw'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical notes\n",
    "\n",
    "Extract information from clinical notes such as the number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non special characters and lower the case\n",
    "wide_df[\"seizure_class_notes\"] = wide_df[\"seizure_class_notes\"].str.replace('[^a-zA-Z]', ' ').str.lower()\n",
    "\n",
    "# Put the number of words in seizure_notes_word_cnt\n",
    "wide_df[\"seizure_notes_word_cnt\"] = wide_df[\"seizure_class_notes\"].str.split().str.len()\n",
    "\n",
    "print(wide_df[\"seizure_notes_word_cnt\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seizure frequency\n",
    "\n",
    "We extract the seizure frequency from a text column (\"seizure_freq) and convert it into a single unit (seizures per month)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def convert_to_monthly(freq_str, age):\n",
    "    # Handling the case when the value is just '0'\n",
    "    if freq_str.strip() == '0':\n",
    "        return 0\n",
    "\n",
    "    # Regular expression to extract numbers and unit, considering various formats\n",
    "    match = re.match(r'(\\d*\\.?\\d+)(?:\\s*-\\s*(\\d*\\.?\\d+))?\\s*(seizures?\\/|seizures?\\s+per\\s+|per\\s+|\\/|\\s+)?\\s*(\\w+)', freq_str, re.IGNORECASE)\n",
    "    if match:\n",
    "        num1, num2, _, unit = match.groups()\n",
    "        num1 = float(num1)\n",
    "        num2 = float(num2) if num2 else num1  # If no range, use the single number\n",
    "\n",
    "        # Calculate the mean if there's a range\n",
    "        number = (num1 + num2) / 2\n",
    "\n",
    "        # Conversion rates to monthly frequency\n",
    "        if unit in ['month', 'monthly', 'mo']:\n",
    "            return number\n",
    "        elif unit in ['day', 'daily']:\n",
    "            return number * 30  # Approximate days in a month\n",
    "        elif unit in ['week', 'weekly']:\n",
    "            return number * 4.345  # Average weeks in a month\n",
    "        elif unit in ['year', 'yearly']:\n",
    "            return number / 12  # Months in a year\n",
    "        elif unit == 'lifetime':\n",
    "            if age > 0:\n",
    "                # Convert lifetime frequency to monthly based on age\n",
    "                return number / (age * 12)\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            # Unknown unit\n",
    "            return None\n",
    "    else:\n",
    "        # Pattern not matched\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the conversion to each row and create a new column\n",
    "wide_df['freq_per_month'] = wide_df.apply(lambda row: convert_to_monthly(row['seizure_freq'], row['age']), axis=1)\n",
    "\n",
    "result = wide_df[[\"freq_per_month\", \"seizure_freq\"]]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how many seizure frequency the RE was able to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wide_df) - wide_df['freq_per_month'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how many seizure frequency after manually checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional cell execution\n",
    "if execute_special_cell:\n",
    "\n",
    "    seizure_freq = wide_df[[\"record_id\", \"freq_per_month\", \"seizure_freq\"]]\n",
    "    seizure_freq = seizure_freq.rename(columns={\"freq_per_month\": \"freq_per_month_hw\"})\n",
    "    seizure_freq.to_csv(\"../../data/processed/freq_per_month_hw.csv\")\n",
    "    \n",
    "else:\n",
    "    # Read the hand-written DataFrame\n",
    "    seizure_freq_df = pd.read_csv(\"../../data/processed/freq_per_month_hw_done.csv\")\n",
    "\n",
    "    # Merge the hand-written data with wide_df\n",
    "    wide_df = pd.merge(wide_df, seizure_freq_df[['record_id', 'freq_per_month_hw']], on='record_id', how='left')\n",
    "\n",
    "    print(len(wide_df) - wide_df['freq_per_month_hw'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date of last surgery\n",
    "\n",
    "As technology changes over time, so will surgery outcome.\n",
    "We extract the year of last surgery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'date' column and extract the year\n",
    "wide_df['eo_year_surg'] = wide_df['eo_date_surg'].str.split('/').str[2]\n",
    "\n",
    "# Convert the 'year' column to numeric (optional, if needed)\n",
    "wide_df['eo_year_surg'] = pd.to_numeric(wide_df['eo_year_surg'])\n",
    "\n",
    "wide_df['eo_year_surg'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remapping sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before, 1 represented female and 2 represented male\n",
    "# Now, 0 represents female and 1 represents male\n",
    "wide_df[\"sex_gender\"] = wide_df[\"sex_gender\"].map({1: 0, 2: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for redundant columns\n",
    "\n",
    "### Check for high correlation variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = wide_df.select_dtypes(exclude=['object']).corr()\n",
    "\n",
    "# Set a threshold for high correlation (e.g., 0.8 for 80%)\n",
    "threshold = 0.8\n",
    "\n",
    "# Find pairs of variables with a very high correlation\n",
    "high_correlation_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            high_correlation_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j]))\n",
    "\n",
    "# Print the pairs of variables with a very high correlation\n",
    "print(f\"Pairs of variables with correlation above {threshold}:\")\n",
    "for pair in high_correlation_pairs:\n",
    "    print(pair)\n",
    "    \n",
    "del correlation_matrix, threshold, high_correlation_pairs, pair, i, j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variance for each column\n",
    "column_variances = wide_df.select_dtypes(exclude=['object']).var()\n",
    "\n",
    "# Set a threshold for low variance\n",
    "threshold = 0.1\n",
    "\n",
    "# Find columns with a variance very close to 0\n",
    "low_variance_columns = column_variances[column_variances < threshold]\n",
    "\n",
    "# Print the count of columns with low variance and their names\n",
    "count_low_variance_columns = len(low_variance_columns)\n",
    "print(f\"Number of columns with variance below {threshold}: {count_low_variance_columns}\\n\")\n",
    "\n",
    "# Print the column names and their variances\n",
    "print(f\"Columns with variance below {threshold} (sorted by variance):\")\n",
    "for column, variance in low_variance_columns.sort_values().items():\n",
    "    print(f\"{column}: {variance}\")\n",
    "    \n",
    "del threshold, low_variance_columns, count_low_variance_columns, column, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count columns with a variance of 0\n",
    "zero_variance_columns_count = (column_variances == 0).sum()\n",
    "\n",
    "# Get the column names with a variance of 0\n",
    "zero_variance_columns = column_variances[column_variances == 0].index.tolist()\n",
    "\n",
    "# Print the count and names of columns with a variance of 0\n",
    "print(f\"Number of columns with a variance of 0: {zero_variance_columns_count}\")\n",
    "print(f\"Columns with a variance of 0:\")\n",
    "for column in zero_variance_columns:\n",
    "    print(column)\n",
    "    \n",
    "del zero_variance_columns_count, column_variances, column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete columns with a variance of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wide_df = wide_df.drop(zero_variance_columns, axis=1)\n",
    "\n",
    "del zero_variance_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a single outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows where all surg_engel___1, 2, 3, 4 variables are 0\n",
    "count_zero_rows = (wide_df[['surg_engel___1', 'surg_engel___2', 'surg_engel___3', 'surg_engel___4']] == 0).all(axis=1).sum()\n",
    "\n",
    "# Display the count of rows with all 0 values\n",
    "print(\"\\nCount of rows with all 0 values:\", count_zero_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows with no engel outcomes (all columns with 0 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where all values are 0\n",
    "wide_df = wide_df[~(wide_df[['surg_engel___1', 'surg_engel___2', 'surg_engel___3', 'surg_engel___4']] == 0).all(axis=1)]\n",
    "\n",
    "# Count rows where all surg_engel___1, 2, 3, 4 variables are 0\n",
    "count_zero_rows = (wide_df[['surg_engel___1', 'surg_engel___2', 'surg_engel___3', 'surg_engel___4']] == 0).all(axis=1).sum()\n",
    "\n",
    "# Display the count of rows with all 0 values\n",
    "print(\"\\nCount of rows with all 0 values:\", count_zero_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1 column out of the 4 dummy variables for engel outcome\n",
    "wide_df['surg_engel'] = wide_df[['surg_engel___1', 'surg_engel___2', 'surg_engel___3', 'surg_engel___4']].idxmax(axis=1).str.split('___').str[1].astype(int)\n",
    "wide_df[[\"record_id\", \"surg_engel\",'surg_engel___1', 'surg_engel___2', 'surg_engel___3', 'surg_engel___4']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe for further use\n",
    "wide_df.to_csv(\"../../data/processed/preprocessed_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export engel outcomes and record_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engel_label_df = wide_df[['record_id', 'surg_engel']]\n",
    "\n",
    "# Save dataframe for further use (i.e. MRI labeling)\n",
    "engel_label_df.to_csv(\"../../data/processed/label_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print system information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "\n",
    "session_info.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
